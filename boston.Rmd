---
title: "Boston House Prices [Unique Version]- Regression Analysis with Machine Learning"
author: "Kar Ng"
date: "2021"
output: 
  github_document:
    toc: yes
    toc_depth: 3
always_allow_html: yes
    
---

***

![](https://raw.githubusercontent.com/KAR-NG/Predicting-House-Prices-in-Boston_UniqueVersion/main/pic1_thumbnail.png)
(*Picture by King of Hearts*)

***

## 1 R PACKAGES

```{r, warning=FALSE, message=FALSE}
# R Libraries

library(tidyverse)
library(skimr)
library(caret)
library(MASS)
library(kableExtra)
library(qqplotr)
library(glmnet)
library(car)
library(corrplot)
library(mgcv)
library(randomForest)
library(doParallel)
library(pls)
library(tidytext)

# R setting

options(scipen = 0)

```

## 2 INTRODUCTION

This project uses a public dataset named "Boston" from a R package - "MASS". It is a famous dataset for machine learning practitioners. However, I have made some adjustments to this dataset to make overall analysis more interesting and unique. 

This Boston dataset studies the effects of a range of variables on median house prices in Boston in late 70s, United States. 

I will statistically analyse the dataset and make predictions with machine learning algorithms. Then, I will study the effects of each variables on the median house prices, pick a model that has the highest predictive power, and build an online interactive application by using RShiny in *section 6 - Model for Production*.


*Highlights of some upcoming graphs*

![](https://raw.githubusercontent.com/KAR-NG/Predicting-House-Prices-in-Boston_UniqueVersion/main/pic2_highlights.png)


## 3 DATA PREPARATION

### 3.1 Data Import

This section imports the edited version of the dataset. Following is 10 rows of data randomly selected from the dataset.

```{r}

boston <- read.csv("boston.csv")
sample_n(boston, 10)

```



### 3.2 Data Description

The dataset has important information regarding factors that may affect the price of a house in Boston in late 70s. For examples, crime rate, number of rooms in the house, nitrogen oxides concentration, its proximity to industrial area, employment centers, highways and etc.

The unit of the house value is "median house price". Believing this median values is extracted from multiple houses in the respective regions in the city.

Following is the data description adapted from the relevant R package. 

```{r}

Variables <- names(boston[2:15])
Description <- c("Per capita crime rate by town.",
                 "Proportion of residential land zoned for lots over 25,000 sq.ft.",
                 "Proportion of non-retail business acres per town.",
                 "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).",
                 "Nitrogen oxides concentration (parts per 10 million).",
                 "Average number of rooms per dwelling.",
                 "Proportion of owner-occupied units built prior to 1940.",
                 "Weighted mean of distances to five Boston employment centres.",
                 "Index of accessibility to radial highways.",
                 "Full-value property-tax rate per per $10,000.",
                 "Pupil-teacher ratio by town.",
                 "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.",
                 "lower status of the population (percent).",
                 "Median value of owner-occupied homes in per $1000s.")

data.frame(Variables, Description) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("hover", "bordered", "stripped"))

```

### 3.3 Data Exploration

The dataset has 506 rows of observations and 14 columns of variables. All variables are in numerical form.

```{r}
glimpse(boston) 

```

From observation, "charles.river" is either 0 or 1, and therefore it should be a categorical variable and it should be assigned with factor type. The first variable, "x", has to be removed because it is just row number and irrelevant.

There are 12 and 17 missing data in the "black" and "lstat" column. Fortunately, the completeness of these two variables are 97.6% and 96.6%. It is a matter of choice whether one wants to remove these values or apply imputation techniques. I will go with the later as it is my purpose to induce these missing values using imputation model using R. 

```{r}
skim_without_charts(boston)


```

Alternatively, I can examine missing data in the dataset by following code. There are 12 missing data in the column "black", and 17 from the column "lstat". 

```{r}
colSums(is.na(boston))

```

Following is a summary summarises some basic descriptive statistics of the dataset.

```{r}
summary(boston)

```
*Insights*

* I can see that there are missing values in the "black" and "lstat" as represented by "NA".

* The feature "charles.river" is a binary type with either 0 or 1.


## 4 DATA CLEANING

This part converts the dataset into a format that is appropriate for analysis or storage.

Depending on context, my usual cleaning techniques include but not limited to the following:

* Renaming of columns and levels if required.  
* Long-wide structure transformation if required.  
* Replacing NA by any appropriate imputation means.          
* Removing variables that do not contribute or irrelevant to the prediction of the responding variable.    
* Removing variables that have too many missing values (generally said a 60% missing values).   
* Removing rows that have many missing values.  
* Factorise variables (or known as features).   
* Feature engineering if required.  

Several specific cleaning tasks identified from the previous section:  

* Remove the first column "x".    
* Convert "charles.river" from integer into factor.      
* Imputation and create new associated features.    


### 4.1 Column removal and factorise

Following code remove first column "X" which is the row number, and factorise the binary column, "charles.river" to convert it into factor. 

```{r}
boston <- boston %>% 
  dplyr::select(-1) %>% 
  mutate(charles.river = as.factor(charles.river))

```


### 4.2 NA Imputation

I am applying *caret* function for this imputation. I will first convert the factor variable "charles river" in the dataset into dummy data as it is required by the package *caret* imputation to work. 

```{r}
# Dummy transformation of factorised Charles river because caret function only work with dummy data.  

dummy.variable <-  dummyVars(~., data = boston[, -14])
boston.dummy <- dummy.variable %>% predict(boston[, -14])

```

Assessing again the number of missing values in this transformed dataset.  

```{r}
colSums(is.na(boston.dummy)) 

```

Now, I am building an imputation model that uses all columns in the dataset to predict these missing values, by the method of "Bagged Decision Trees".

```{r}

# Imputation

imputation.model <- preProcess(boston.dummy, method = "bagImpute")
imputed.boston <- imputation.model %>% predict(boston.dummy)

```

And the missing values have now been filled up by imputation.

```{r}
colSums(is.na(imputed.boston))

```

Over-writing the original "black" and "lstat" with the newly imputed "black" and "lstat" columns.

```{r}

boston$black <- imputed.boston[, 13]
boston$lstat <- imputed.boston[, 14]

```

Now, there are no more missing values in the dataset.

```{r}
colSums(is.na(boston))

```

Since the purpose of this project is to assess the effects of each of these variables in predicting the median house prices. I will keep all variables at this point. I will make variable selection again in later analysis. 

Let's have a final glimpse of the data again.

```{r}
# Convert 2 integers variable "highway.index" and "property.tax " to double type. 

boston <- boston %>% 
  mutate_if(is.integer, as.double)

glimpse(boston)

```

Perfection. 


## 5 EXPLORATORY DATA ANALYSIS (EDA)

Transforming the data frame for effective EDA.

```{r, warning=FALSE}

bos <- boston %>% 
  gather(key = "key", value = "value") %>% 
  mutate(value = as.numeric(value))

```

### 5.1 Distribution Study

This section studies data distribution of each variables. In general, any non-skewed distribution that closes to a Gaussian distribution would be more useful for the prediction of the house value. They would have good relation with the responding variable - "house.value". 

```{r, fig.width=8, fig.height=6}

ggplot(bos, aes(x = value, fill = key)) +
  geom_histogram(colour = "white", bins = 20) +
  facet_wrap(~ key, scale = "free") + 
  theme(legend.position = "none")

```

Insight:

* The *charles.river* has a binary distribution with value in either 1 or 0.   
* The *room* has a distribution that closes to Gaussian distribution.  
* The y variable *house.value* has a nearly Gaussian distributed distribution which is a good sign.   
* Many variables except *charles.river* and *room* seems to have skewed to both directions.      

This part would help in manual selection of features when trying to make the prediction based on traditional linear regression that sensitive to outliers. Outliers are probably the reasons causing these skews. However, choosing a type of machine learning model that robust to outliers, such as decision tree, will be an alternative powerful option.  


### 5.2 Outliers Detection

This section uses boxplot as an alternative visualization of outliers.

```{r, fig.width=10, fig.height=6}

ggplot(bos, aes(x = value, fill = key)) +
  geom_boxplot() +
  facet_wrap(~ key, scale = "free") +
  theme(legend.position = "none")


```

Outliers exists in many variables include *black, crime.rate, dist.to.work, lstat, house.value, ptratio, resid.zone,* and even in the Gaussian distributed *room*。

The distribution plots and box plots show that the assumptions of linear regression have been violated and therefore a non-linear regression would perform better than linear regression algorithm, such as KNN and tree-base algorithms. 


### 5.3 Relationships 

In relation to median house prices, visualisation shows that:

```{r, fig.height=12, fig.width=10, warning=FALSE, message=FALSE}

bos2 <- boston %>% 
  mutate(charles.river = as.numeric(charles.river)) %>% 
  pivot_longer(c(1:13), names_to = "variable", values_to = "result") %>% 
  arrange(variable)

# plot

ggplot(bos2, aes(x = result, y = house.value, colour = variable)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x", ncol = 3) +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 14, hjust = 0.5, vjust = 2),
        strip.text = element_text(size = 10)) +
  geom_smooth(se = F) +
  labs(x = "Variables",
       y = "Median House Price, /$1000",
       title = "The Impact of Environmental Features on Median House Prices")
  


```

Insights:

* The variables that related (either positive or negative) the most with the median house prices are "*room*", "*lstat*", "*pt.ratio*", and probably "*resid.zone*".  

* The relationships of other variables with the median house price are not as much.

* The relationships between the median house prices and each of the independent variables are not linear. 


### 5.4 Multicollinearity

This section tests for the existence of multicollinearity within the dataset. It is a problem when two or more predictor variables are correlated with each other. One of the assumption of linear regression is to have predictor variables independent from each other. Multicollinearity can violate this assumption. 

The existence of multicollinearity will increase the standard errors of coefficients estimates in the linear regression model, eventually alter the 95% Confidence intervals and finally affect the accuracy of P-values of each variables in relation to the median house prices. These P-values are the statistical metrics that we use to evaluate the significance of relationships between predictors with the median house prices.

A correlogram is graphed to study the interaction between each pair of the independent variables. 

```{r, fig.width=10, fig.height=8}

boston_cor <- boston %>% 
  mutate(charles.river = as.numeric(charles.river)) %>% 
  cor()

corrplot::corrplot(boston_cor, method = "number", type = "lower")


```

Generally speaking, an absolute correlation value that is around and higher than 50% indicates a moderate correlation. The closer to 1 in either positive or negative direction, the higher the relationship between the two variables. As a rule thumb, multicollinearity problem is an issue when the correlation between the two independent variables exceed 0.8 or -0.8. 

Therefore, multicollinearity issue is likely to happen between "highway index" and "property tax". They have a correlation degree of 0.91. To avoid multicollinearity problem, one should avoid the coexistence of the two variables in a model. This can be achieved by manual selection or auto-selection by machine learning algorithms. Alternatively, a machine learning algorithm that immune to multicollinearity should be selected during model building. 

An alternative, popular multicollinearity detection method, called variance inflation factor (VIF) will be performed in next section. This method requires a model to be built prior to evaluation. VIF will further confirm the result of multicollinearity detection by this correlogram.  


## 5 Model Building

### 5.1 Train-test split

This section creates data partitions into 80% of train set and 20% test set. The train set will be used to build models and the test set will be used to evaluate the performance of each models.

```{r}
set.seed(123)

# Create data partition

train.index <- boston$house.value %>% createDataPartition(p = 0.8, list = F)

# Get train and test set

train.data <- boston[train.index, ]
test.data <- boston[-train.index, ]
  
```


### 5.2 Multiple Linear Regression (MLR)

A multiple linear regression is created to study the effect of each variables on the median house prices.

```{r}

# Using original dataset to include all data into this model

model_mlr <- lm(house.value ~., data = train.data)

```

As mentioned, a variance inflation factor (VIF) evaluation is performed. A VIF that exceeds 5 indicates a problematic amount of collinearity (James et al. 2014). The result matches the outcome from the previous correlogram that "highway.index" and "property.tax" correlated and should not be coexist in a linear regression model to avoid the issue of multicollinearity.

```{r}

vif(model_mlr)

```


Following are two linear models built with the exception of either highway.index or property.tax. Their adjusted R-Squared will be compared and the model with the higher adjusted R-squared will be selected.

```{r}
set.seed(123)

model2 <- lm(house.value ~. - highway.index , data = train.data)

summary(model2)

```

```{r}
set.seed(123)

model3 <- lm(house.value ~. - property.tax , data = train.data)

summary(model3)

```
The model without property.tax (model 3) has higher adjusted R-squared at 0.708 as compared to 0.7012 of the previous model without highway.index. Additionally, the RSE of this model (model 3) is 4.954 which is lower than the previous model (model 2) built with property tax at 5.012. Therefore, property.tax should be dropped to avoid multicollinearity.   

**Model performance**

* P-value of the F-Statistics of this model is < 0.05, indicating that there is at least of the predictor variable is significantly related to the median house prices.  

* The adjusted R-squared of this model is 0.7091, which is a good value indicating that this multiple linear regression model is able to explain 70.91% of the variation in the median house prices. 

* The Residual standard error (RSE) is 4.954, This corresponds to an error rate of 22%, which is acceptable but high enough to investigate a better model for prediction.  
 
```{r}

4.954/mean(Boston$medv)

```

**Insights from coefficient estimates**

Following visualisation indicates the strength of coefficients and significance level of each variable in relation to the median house price.  

```{r, fig.width=8, fig.height=5}

# set up df

coef_plot <- data.frame(summary(model3)$coef) %>% 
  rownames_to_column(var = "variable") %>% 
  filter(variable != "(Intercept)") %>% 
  rename(P_value = "Pr...t..") %>% 
  mutate(sig = case_when(P_value < 0.05 & P_value > 0.01 ~ "*",
                         P_value < 0.01 & P_value > 0.001 ~ "**",
                         P_value < 0.001 ~ "***",
                         TRUE ~ " "))

# plot

plot_mlr <- ggplot(coef_plot, aes(y = Estimate, x = fct_reorder(variable, -Estimate), fill = variable)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 20, size = 10, hjust = 0.7),
        plot.margin = unit(c(1,1,1,1), "cm")) +
  geom_text(aes(label = paste0("(", round(Estimate, 2), ")")), vjust = 1) +
  geom_text(aes(label = sig), size = 8) +
  labs(x = "Variables", 
       y = "Coefficient Estimate",
       subtitle = "*: P<0.05, **: P<0.01, ***: P<0.001",
       title = "Coefficient of Variables in Relation to Median House Prices")+
  scale_y_continuous(lim = c(-20, 5), breaks = seq(-20, 5, 5))


plot_mlr

```
Insights from this section: 

* Variables "resid.zone", "age", and "indus.biz" do not have significant relationship with the median house prices.

* Variable "room" has the highest significant positive impact on median house prices.

* Variable "nitrogen oxide" has the highest significant negative impact on median house prices. 


### 5.3 Assumption tests of Multiple Linear Regressions

This section describes whether the assumptions of linear regression is fulfilled.

* The relationship between independent variables and the responding variable is non-linear. 


```{r}

plot(model3, 1)

```

* The second assumption is that independent variables should be independent from each other. It is not the case in this dataset as there is multicollinearity problem between "rad" and "tax".  

* Following plot the standardised residual against the fitted values shows that the amount of error is not similar at each data point of the linear model and therefore it has a feature of heteroscedasticity. 


```{r}

plot(model3, 3)

```
* There is also no multivariate normality shown by a standard Q-Q plot below formed by the multiple linear regression model. An ideal trend would be having all points falling near the straight line in the middle of the plot and form a line. 

```{r}
plot(model3, 2)
```

This dataset has a non-parametric characteristics and therefore a non-parameter machine learning prediction model should be selected. 


### 5.4 Lasso

This section I will still apply a parametric algorithm, Lasso regression, to check out how would the L1-norm lambda regularisation of this method performs. The performance metrics of this method will be used as a baseline model to compared with non-parametric machine learning algorithms that I am going to apply later.

```{r, warning=FALSE}

set.seed(123)

# build the model

model_lasso <- train(house.value ~., data = boston,
                     method = "glmnet",
                     trControl = trainControl(method = "repeatedcv",
                                              number = 10,
                                              repeats = 3),
                     tuneGrid = expand.grid(alpha = 1,
                                            lambda = 10^seq(-3, 3, length = 100)))

```

From above lasso model, I built a grid that contains many lambda value, and it seems like the best one will be closing to 0. A best lambda value is the one that having the lowest RMSE.

```{r}
plot(model_lasso)

```

R *caret* package automatically identified the best lambda for me which is 0.0284.

```{r}
model_lasso$bestTune

```

Interesting, many materials state that Lasso regression should be used when there is multicollinearity issue in the dataset because Lasso will select 1 variable from the highly correlated group. However, it is not happening in my case. The "highway.index" and "property.tax" are correlated with a VIF above 5 and therefore they should not coexist in a model as it would affect the accuracy of the coefficients. 

```{r}
coef(model_lasso$finalModel, model_lasso$bestTune$lambda)

```

It is likely that a the best lambda is determined based on the lowest point of RMSE. Therefore, I initiate a trade-off here. I manually selected the lambda value that will exclude either one from the high correlated group.


```{r}

# build the model

model_lasso <- train(house.value ~., data = boston,
                     method = "glmnet",
                     trControl = trainControl(method = "repeatedcv",
                                              number = 10,
                                              repeats = 3),
                     tuneGrid = expand.grid(alpha = 1,
                                            lambda = 0.19)) # I manually found that lambda 0.19 have property.tax removed that is highly correlated with highway.index


coef(model_lasso$finalModel, model_lasso$bestTune$lambda)

```
Following compute the R2 and RMSE of Lasso from predicting the test data. It will be recorded in the last section for a grand model comparison. 

```{r}

# predictions

prediction_lasso <- model_lasso %>% predict(test.data)

# model performance

R2_lasso <- caret::R2(prediction_lasso, test.data$house.value)
RMSE_lasso <- RMSE(prediction_lasso, test.data$house.value)

R2_lasso
RMSE_lasso

```



### 5.5 PLS

A principle component technique is applied in this section, called Partial Least Squares (PLS). PLS is a method that can avoid multicollinearity between PLS summarises the predictors into a few new variables called principle component. Then, these new variables are fit to linear regression model. 

```{r}

set.seed(123)

model_pls <- train(house.value ~., data = train.data,
                   method = "pls",
                   scale = TRUE,
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10, 
                                            repeats = 3),
                   tuneLength = 10)

```

This is a plot showing how are the numbers of principal components (PCs) in relation to RMSE. The lower the RMSE, the lower the model. The optimum level of PC should be around 8 to 10.

```{r}
plot(model_pls)

```

R can help to identify the best number of components (ncomp) to be used in the model, which is 9. This value will be automatically set as the default "ncomp" to be used during prediction. 

```{r}
model_pls$bestTune
```

Internally, this model captures 90.89% of variation in the 9 components and 72.21% of the outcome variation.

```{r}
summary(model_pls$finalModel)
```

In predicting the test variable, PLS has done a little better job than lasso in terms of R2 and RMSE. These metrics will be recorded for final grand comparison. 

```{r}

# Predictions

predictions_pls <- model_pls %>% predict(test.data)

# Model performance

caret::R2(predictions_pls, test.data$house.value)
caret::RMSE(predictions_pls, test.data$house.value)


```

### 5.6 KNN

K-Nearest Neighbors (KNN) is the very first non-parametric model to be implemented. It does not need to comply with parametric assumptions as well as the collinearity issue. This method uses the neighboring points to make estimations during prediction. 

```{r}

model_knn <- train(house.value ~., data = boston,
                   method = "knn",
                   trControl = trainControl(method = "repeatedcv", 
                                            number = 10,
                                            repeats = 3),
                   preProcess = c("center", "scale"),
                   tuneLength = 10
                   )

```

Grabbing the nearest 5 neighboring points during internal estimation and prediction had the lowest RMSE. Therefore, it is the optimum "k" nearest point that this model will be using during prediction on the test dataset. 

```{r}
plot(model_knn)

```

```{r}
model_knn$bestTune

```

Great, the result has an significant improvement from the parameter options. 

```{r}
# Predictions

predictions_knn <- model_knn %>% predict(test.data)

# Model performance

caret::R2(predictions_knn, test.data$house.value)
caret::RMSE(predictions_knn, test.data$house.value)

```


### 5.7 Decision Tree / CART

Decision tree, also known as CART (Classification and Regression Tree) is my second non-parametric machine learning algorithm that work should very well in condition when there is highly non-linear relationship between the predictors and the responding variable. Decision tree has a upside-down tree-like structure with decision rules in each of the branch to guide the prediction of new observations. 

I am expecting a better result because decision tree tend to prefer a situation that a few of variables in the variables list are more powerful than the others. This house dataset has this characteristic. Few of the powerful features are nitrogen.oxide, room, and charles.river. This method is also immune to multicollinearity.


```{r, warning=FALSE}

set.seed(123)

model_DT <- train(house.value ~., data = train.data,
                  method = "rpart", 
                  trControl = trainControl(method = "repeatedcv", 
                                            number = 10,
                                            repeats = 3),
                  tuneLength = 10)   # Specifying the number of possible cp values for pruning, default is 3. 

```

Complexity parameter (CP) is the index set to prune the tree to avoid overfitting. The best CP detected is 0.0086 with the lowest RMSE.

```{r}
plot(model_DT)

```


```{r}
model_DT$bestTune

```

Following decision tree can help to visualise the important decisions to be made during each split. 

```{r}

par(xpd = NA)
plot(model_DT$finalModel, uniform = T, branch = 0.5, compress = T)
text(model_DT$finalModel, col = "darkgreen", fancy = F)

```

Decision tree specifies the variable "room" as the root node (the node at the very top) after trying out all variables. On the root node, it appears that "room" leads to the purest two branches compared to other variables. Formally, It means that the information gain for the variable "room" is the highest. From the root node, we will know more about median house prices after looking at "room" compared to other variables. In regression decision tree, the value of each split cutoff point (for example "room < 6.838") is selected to result the purest branch, or defined that residual sum of squared error (RSS) is minimized. 
 
This computation is iterated at each resulting branches on sub-samples resulting from individual parent nodes until either maximum depth of the tree or when pure branches are achieved. This process is known as recursive partitioning.

To improve the visual, following is an alternative visual showing the exact same information as above tree however it shows the number of samples and associated proportion during each split. 

```{r}


library(rattle)
fancyRpartPlot(model_DT$finalModel, palettes = "Oranges")


```

Following is the decision rules defined in the model. 

```{r}
model_DT$finalModel
```

```{r}
# predictions

prediction_DT <- model_DT %>% predict(test.data)

# Model performance

caret::R2(prediction_DT, test.data$house.value)
caret::RMSE(prediction_DT, test.data$house.value)

```

Decision tree is a powerful machine learning algorithm. However, it do come with its disadvantages. It is because that only one tree is built and therefore its result is highly relied on the training set that used in the split of the decision tree. Therefore, there might be significant impact on the tree if there is a small change in the dataset. The lower result above (79.58%) might be because that the decision tree was not generalise well and overfit the training data. 

To solve this problem, next section I will build and aggregate many trees for do a better prediction, it is known as random forest. I will hope to see a boost in prediction accuracy on the test dataset. 


### 5.8 Random Forest

Random forest, is also known as an ensemble learning or method because it aggregates many of decision trees, averaging the internally built models, and creating a final high-performance predictive model compared to decision tree. 

This section will grow a high "data-diversity" random forest like many other projects in the world that used Random forest. All trees will not be completely the same. It is because of 2 popular techniques (James et al. 2014, P. Bruce and Bruce 2017): 

* (1) each of the tree randomly grabs a data subset from the input training dataset and averaging the models' results, it is known as *bagging* or *bootstrap aggregating*) and, 

* (2) each of the tree will be given randomly a smaller number of variables for them to choose when they do their split in their nodes, it is known as *bootstrap sampling*.


```{r}
set.seed(123)

model_rf <- train(house.value ~., data = train.data,
               method = "rf",
               trControl = trainControl(method = "repeatedcv",
                                        number = 10,
                                        repeats = 3), 
               importance = T)

model_rf$finalModel

```

By default, 500 trees were grown to build the random forest model with their results averaged. The internal accuracy is 83.29%, which is considered very good. You will see a much better at 89.9% (or 90%) later when I predict on the test dataset. 

The optimum number (mtry) of predictor variables randomly selected as variables of choice during each split is 7. Ths value is automatically discovered by the *caret* package.  

```{r}
model_rf$bestTune

```

The prediction result of this random forest model with the test data is:

```{r}

# Make predictions

predictions <- model_rf %>% predict(test.data)

# Model performance

caret::R2(predictions, test.data$house.value)
caret::RMSE(predictions, test.data$house.value)

```

Plotting the variance importance plot of random forest and having following result. This "importance" plot tells you which variables are importance in term of their predictive power in relative to each other. The higher the importance ranking of one variable in the importance plot, the higher its impact on the outcome variables. It is also associated with an increase in significance level (lower P-value). 

```{r}
plot(varImp(model_rf))

```

This importance plot created by the random forest algorithm is useful in telling us how far the important features are away from the unimportant ones. I can see that "lstat" which is the proportion of lower status of the population in the community and the number of room by "room" are the two most important features in predicting the median house prices. 

This result is the same as the results from multiple regression regression in previous section 5.2, check out following summary tabels. 

```{r}
options(scipen = 999)

summary(model_mlr)$coef %>% 
  data.frame() %>% 
  rename(P_Value = Pr...t..) %>% 
  arrange(P_Value) %>% 
  dplyr::select(Estimate, P_Value) %>%  
  mutate(Significance.ordered = case_when(P_Value < 0.05 & P_Value > 0.01 ~ "* (< 0.05)",
                                  P_Value < 0.01 & P_Value > 0.001 ~ "** (< 0.01)",
                                  P_Value < 0.001 ~ "*** (< 0.001)",
                                  TRUE ~ " ")) %>% 
  arrange(P_Value) %>% 
  rownames_to_column(var = "features") %>%
  mutate("no." = row_number()) %>% 
  relocate("no.", .before = features) %>% 
  filter(features != "(Intercept)",
         Estimate > 0) %>% 
  kbl(caption = "Factors that Negatively Correlated with House Prices") %>% 
  kable_paper() 
  

options(scipen = 1)

```

```{r}
options(scipen = 999)

summary(model_mlr)$coef %>% 
  data.frame() %>% 
  rename(P_Value = Pr...t..) %>% 
  arrange(P_Value) %>% 
  dplyr::select(Estimate, P_Value) %>%  
  mutate(Significance.ordered = case_when(P_Value < 0.05 & P_Value > 0.01 ~ "* (< 0.05)",
                                  P_Value < 0.01 & P_Value > 0.001 ~ "** (< 0.01)",
                                  P_Value < 0.001 ~ "*** (< 0.001)",
                                  TRUE ~ " ")) %>% 
  arrange(P_Value) %>% 
  rownames_to_column(var = "features") %>%
  mutate("no." = row_number()) %>% 
  relocate("no.", .before = features) %>% 
  filter(features != "(Intercept)",
         Estimate < 0) %>% 
  kbl(caption = "Factors that Negatively Correlated with House Prices") %>% 
  kable_paper() 
  

options(scipen = 1)

```

* Importance plot shows that "room" is the most important variable, which is supported by multiple linear regression summary with indication that this relationship is negative. 

* Importance plot shows that "lstat" is the second most important variable, which is supported by multiple linear regression summary with indication that this relationship is positive. 


The results of these two different algorithms usually are not the same. For example, the relationships between features and the responding variables in this project is non-linear and the result from multiple linear regression may not be so correct as compared to random forest that can digest non-linearity. Results from both algorithm could be similar (not entire the same) only if predictive power among features are really outcompete others in a significant way.

Variables "lstat" and "room" are instead supported by the results of the two different algorithms, and therefore it is robust to say "lstat" and "room" are the two most important features in the prediction of median house prices.

### 5.9 Stochastic gradient boosting (*XGBoost* in R)

This section builds an alternative forest, known as "XGBoost". Compared to random forest, many trees will also be grown in XGBoost "forest". However, trees are now grown sequentially, one by one, by using information from previously grown trees with an aim to minimise the error from the previous models (James et al. 2014). Therefore, trees will become better and better with lesser and lesser error. 

In the following, I am applying the *caret* package to combine with the *xgboost* package to automatically find the best tuning parameters and fit the final best boosted tree.  

```{r}
set.seed(123)

model_xgb <- train(house.value ~., data = train.data,
                   method = "xgbTree",
                   trControl = trainControl(method = "repeatedcv",
                                        number = 10,
                                        repeats = 3) 
                   )

```

This *caret* codes help to search a numbers of tuning parameters as shown below.

```{r}
model_xgb

```

The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight
 = 1 and subsample = 1.

```{r}
model_xgb$bestTune

```

Making prediction on the test data using this xgb_model. 

```{r}

# predictions

predictions <- model_xgb %>% predict(test.data)

# model performance

caret::R2(predictions, test.data$house.value)
caret::RMSE(predictions, test.data$house.value)

```

This model has results that are not too far away from random forest. Next section will create a graph to compare the performance metrics of all machine learning models I have trained.



### 6.0 Final Model Comparison

```{r, fig.width=9}

# set up dataframe

Model <- c("Lasso", 
           "PLS",
            "KNN",
           "Decision Tree",
           "Random Forest",
           "XGBoost")

R2_value <- c(0.7652148,
              0.7687147,
              0.8841925,
              0.7953629,
              0.8996801,
              0.8844266)
              
RMSE_value <- c(4.663365,
                4.531626,
                3.46252,
                4.217293,
                3.038207, 
                3.190433
                )

models <- data.frame(Model, R2_value, RMSE_value)

models <- models %>% 
  mutate(R2_percentage = round(R2_value*100, 2),
         Error_rate_percent = RMSE_value/mean(test.data$house.value),
         Error_rate_percent = round(Error_rate_percent * 100,2)) %>% 
  pivot_longer(c(4:5), names_to = "metrics", values_to = "results") %>% 
  mutate(Model = reorder_within(x = Model, by = results, within = metrics))
  

# plot

ggplot(models, aes(x = fct_reorder(Model, -results), y = results, fill = metrics)) +
  geom_bar(stat = "identity") +
  facet_wrap(~metrics, scale = "free_x") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 10),
        plot.title = element_text(face = "bold")) +
  scale_x_reordered() +
  labs(x = "Models",
       y = "%",
       title = "Comparing Performance Metrics of All ML Models") +
  geom_text(aes(label = paste0(results, "%"), vjust = 1.5))

```

From this result, I conclude that random forest algorithm has the highest prediction performance on the randomly sampled test dataset in this project. It has the highest good R-squared value (R2, %) at 89.97%, or 90%. 

The 89.97% R-squared value means that the predicted outcome values by the Random Forest using the test dataset has a high correlation with the observed values in the test dataset. Alternatively, results are approximately 90% similar. Random forest has also the lowest prediction error rate at 13.43% (RMSE divided by the mean of the y variables in test dataset).



## 6 Model for Production

This section uses RShiny to produce an online interactive application to make predictions using the random forest algorithm. 

Demo picture:

![](https://raw.githubusercontent.com/KAR-NG/Predicting-House-Prices-in-Boston_UniqueVersion/main/pic4_shiny.JPG)


**Visit this link to use the app:**

https://karhou.shinyapps.io/boston/

**Visit this github link to view the codes I used to program this app.**

https://github.com/KAR-NG/Predicting-House-Prices-in-Boston_UniqueVersion/blob/main/app.R




## 7 Conclusion

In conclusion, 7 different models were built to study this dataset, included multiple linear regression (MLR), Lasso regression, Partial Least Squares (PLS), K-Nearest Neighbor (KNN), Decision tree, random forest, and stochastic gradient boosted random forest (XGBoost). 

* Random forest model had the best predictive power at 90% compared to all other models and should be used for prediction. A RShiny app has been built to make this model into production. 

* The variable "*age*" that stands for "Proportion of owner-occupied units built prior to 1940" and the variable "*indus*" that stands for "Proportion of non-retail business acres per town" are **not related** in the house prices with P higher than 0.05. 

* The **3 most positively related** variables are the *number of rooms* that affects house prices the most with the most significance level (lowest P-value with a value of < 0.001), followed by the *proportion of black community* (P-value <0.001) and the *index of accessibility to radial highway* (P-value <0.001).

* The **3 most negatively related** variables are the *lower status of the population* (percent) (P-value <0.001), followed by the second ranked *nitrogen oxide concentration* (P-value <0.001), and the third negative variable is *crime rate* (P-value < 0.05). The higher the values of these variables, house prices are negatively impacted the most.

* In overall, the most important 2 variables related to the Boston median house prices in the late 70s are the number of room (positive related) and the proportion of lower status population in the community (negative related). 


*Thank you for reading*

## 8 LEGALITY  

The purpose of this project is for educational and skills demonstration ONLY. 


## 9 REFERENCE 

Boston thumbnail picture By King of Hearts - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=62981160

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.

Brownlee J 2016, *How to Work Through a Regression Machine Learning Project in Weka*, viewed 26 September 2021, https://machinelearningmastery.com/regression-machine-learning-tutorial-weka/ 

Data Professor 2020, *Web Apps in R: Building the Machine Learning Web Application in R | Shiny Tutorial Ep 4*, viewed 4 October 2021, https://www.youtube.com/watch?v=ceg7MMQNln8&t=847s 

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R . Springer Publishing Company, Incorporated.

Minitab Blog Editor 2013, *Enough Is Enough! Handling Multicollinearity in Regression Analysis*, viewed 25 September 2021, https://blog.minitab.com/en/understanding-statistics/handling-multicollinearity-in-regression-analysis

Sivakumar C 2017, *https://rpubs.com/chocka314/251613*, viewed 28 September 2021, https://rpubs.com/chocka314/251613

https://cran.r-project.org/web/packages/MASS/MASS.pdf
